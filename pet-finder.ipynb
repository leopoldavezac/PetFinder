{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.7.10","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# Librairies","metadata":{}},{"cell_type":"code","source":"from typing import List, Union, Tuple, Callable, Dict\n\nfrom os import environ\n\nfrom random import seed\n\nfrom numpy.random import seed as np_seed\nfrom numpy import ndarray, zeros\n\nfrom pandas import DataFrame, read_csv\n\nfrom sklearn.model_selection import train_test_split\n\nimport tensorflow as tf\nfrom tensorflow import random\nfrom tensorflow import keras\nfrom tensorflow.keras import applications\nfrom tensorflow import data\n\nimport plotly.graph_objects as go\n\nimport wandb\nfrom wandb.keras import WandbCallback\n\nfrom kaggle_secrets import UserSecretsClient","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# WandB","metadata":{}},{"cell_type":"code","source":"user_secrets = UserSecretsClient()\napi_key = user_secrets.get_secret(\"WANDB\")\nwandb.login(key=api_key)\n\nrun = wandb.init(\n    project=\"pet_finder\",\n    entity=\"leopoldavezac\",\n    config={\n        'learning_rate':0.001,\n        'epochs':20,\n        'batch_size':24,\n        'loss_func':'mse',\n        'img_width':224,\n        'img_length':224,\n        'efficient_net_symbol':'B0',\n        'efficient_net_trainable':False,\n        'dense_layers_post_efficient_net':[18, 9],\n        'dropout':0.2,\n        'data_augmentation_contrast':0.1,\n    }\n)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Constants","metadata":{}},{"cell_type":"code","source":"DATA_PATH = '../input/petfinder-pawpularity-score'\n\nID_VAR_NM = 'Id'\nTARGET_VAR_NM = 'Pawpularity'\n\nAUTOTUNE = tf.data.experimental.AUTOTUNE\n\nCONFIG = wandb.config","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Load & Preprocess Data","metadata":{}},{"cell_type":"code","source":"\ndef get_datasets() -> List[tf.data.Dataset]:\n\n    df_train = load_df(set_nm='train')\n    df_test = load_df(set_nm='test')\n\n    df_train[TARGET_VAR_NM] /= 100\n\n    df_train = create_img_path_var(df_train, 'train')\n    df_test = create_img_path_var(df_test, 'test')\n\n    df_train, df_val = split(df_train)\n\n    ds_train = create_dataset_with_preprocessed_imgs(\n        df_train.img_path.values,\n        df_train[TARGET_VAR_NM].values.astype('float'),\n        augment=True\n        )\n    ds_val = create_dataset_with_preprocessed_imgs(\n        df_val.img_path.values,\n        df_val[TARGET_VAR_NM].values.astype('float')\n        )\n    ds_test = create_dataset_with_preprocessed_imgs(\n        df_test.img_path.values\n        )\n\n    return [ds_train, ds_val, ds_test]\n\n\ndef load_df(set_nm:str) -> DataFrame:\n\n    var_nms = [ID_VAR_NM] \n    var_nms += [TARGET_VAR_NM] if set_nm == 'train' else []\n\n    return read_csv('{}/{}.csv'.format(DATA_PATH, set_nm), usecols=var_nms)\n\n\ndef create_img_path_var(df: DataFrame, set_nm:str) -> DataFrame:\n\n    df['img_path'] = '{}/{}/'.format(DATA_PATH, set_nm) + df[ID_VAR_NM] + '.jpg'\n    df.drop(columns=ID_VAR_NM, inplace=True)\n\n    return df\n\n\ndef split(df: DataFrame) -> List[DataFrame]:\n\n    train, val = train_test_split(df.values, test_size=0.2)\n\n    df_train = DataFrame(train, columns=df.columns)\n    df_val = DataFrame(val, columns=df.columns)\n\n    return [df_train, df_val]\n\ndef create_dataset_with_preprocessed_imgs(X_paths: ndarray, y: Union[None, ndarray] = None, augment:bool=False) -> data.Dataset:\n\n    get_preprocessed_img = build_img_processor(y is not None)\n    \n    if y is not None:\n        ds = data.Dataset.from_tensor_slices((X_paths, y))\n    else:\n        ds = data.Dataset.from_tensor_slices((X_paths,))    \n    \n    ds = ds.map(get_preprocessed_img, num_parallel_calls=AUTOTUNE)\n    \n    if augment:\n        augmentation_model = get_augmentation_model()\n        ds = ds.map(lambda X, y: (augmentation_model(X, training=True), y))\n    \n    ds = ds.batch(CONFIG.batch_size)\n    ds = ds.prefetch(buffer_size=AUTOTUNE)\n    \n    return ds\n\ndef build_img_processor(with_target: bool) -> Callable:\n    \n    def get_preprocessed_img(path: str) -> tf.Tensor:\n\n        img = load_img(path)\n        img = resize(img)\n        img = eff_net_preprocess(img)\n\n        return img\n    \n    def get_preprocessed_img_with_target(path:str, y:float) -> Tuple[Union[tf.Tensor, float]]:\n        \n        return (get_preprocessed_img(path), y)\n    \n    return get_preprocessed_img_with_target if with_target else get_preprocessed_img\n\ndef load_img(path: str) -> tf.Tensor:\n\n    img = tf.io.read_file(path)\n    return tf.io.decode_jpeg(img)\n\ndef resize(img: tf.Tensor) -> tf.Tensor:\n\n    return tf.cast(\n        tf.image.resize_with_pad(img, CONFIG.img_length, CONFIG.img_width),\n        dtype=tf.int32\n        )\n\ndef eff_net_preprocess(img: tf.Tensor) -> tf.Tensor:\n    \n    return keras.applications.efficientnet.preprocess_input(img)\n\ndef normalize(img: tf.Tensor) -> tf.Tensor:\n    \n    return img / 255.0\n\ndef get_augmentation_model() -> tf.keras.Model:\n    \n    return tf.keras.Sequential([\n      layers.RandomFlip(\"horizontal\"),\n      layers.RandomRotation(CONFIG.data_augmentation_contrast),\n    ])\n\n    ","metadata":{"_kg_hide-input":true,"_kg_hide-output":true,"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"ds_train, ds_val, ds_test = get_datasets()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Img Dims Visualization","metadata":{}},{"cell_type":"code","source":"img_paths = ('{}/{}/'.format(DATA_PATH, 'train') + load_df('train')[ID_VAR_NM] + '.jpg').values\nimg_dims = zeros((len(img_paths), 2))\n\nfor i, img_path in enumerate(img_paths):\n    \n    img_dims[i,:] = load_img(img_path).shape[:-1]\n    \n\nfig = go.Figure()\nfig.add_trace(go.Histogram(x=img_dims[:,0], histnorm='probability', name='width'))\nfig.add_trace(go.Histogram(x=img_dims[:,1], histnorm='probability', name='height'))\nfig.update_layout(title_text='Distribution of Img Width and Height')\nfig.show()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Model","metadata":{}},{"cell_type":"code","source":"def get_model(efficient_net_model_nm:str, dense_layers_post_eff_net:List[int], dropout: float) -> tf.keras.Model:\n    \n    \n    efficient_net = tf.keras.models.load_model('../input/keras-applications-models/{efficient_net_model_nm}.h5')\n    \n    if CONFIG.efficient_net_trainable:\n        unfreeze_layers(efficient_net)\n    \n    layers = [\n            tf.keras.layers.Input(shape=(CONFIG.img_length, CONFIG.img_width, 3)),\n            efficient_net,\n            tf.keras.layers.BatchNormalization(),\n            tf.keras.layers.Dropout(dropout)\n        ]\n        \n    layers += [tf.keras.layers.Dense(nb_units) for nb_units in dense_layers_post_eff_net]\n    \n    layers += [tf.keras.layers.Dense(1, activation='sigmoid')]\n    \n    model = keras.models.Sequential(layers)\n    \n    print(model.summary())\n    \n    return model\n\ndef unfreez_layers(model: tf.keras.Model) -> None:\n    \n    for layer in model.layers:\n        if not isinstance(layer, tf.keras.layers.BatchNormalization):\n            layer.trainable = True\n        else:\n            layer.trainable = False\n    \n\ndef compile_model(model: keras.Model, learning_rate: float, loss_func:str) -> None:\n    \n    optimizer = keras.optimizers.Adam(learning_rate=learning_rate)\n    \n    model.compile(loss=loss_func, optimizer=optimizer, metrics=[keras.metrics.RootMeanSquaredError()])\n\n\ndef fit(model: keras.Model, ds_train: data.Dataset, ds_val: data.Dataset, epochs: int) -> None:\n    \n    early_stopping = tf.keras.callbacks.EarlyStopping(monitor='val_loss', patience=3)\n\n    model.fit(ds_train, epochs=epochs, validation_data=ds_val, callbacks=[WandbCallback(), early_stopping])","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"\nmodel = get_model(CONFIG.efficient_net_symbol, CONFIG.dense_layers_post_efficient_net, CONFIG.dropout)\ncompile_model(model, CONFIG.learning_rate, CONFIG.loss_func)\nfit(model, ds_train, ds_val, CONFIG.epochs)\n\nrun.finish()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Submissions","metadata":{}},{"cell_type":"code","source":"\ndef save_test_pred(pred: ndarray) -> None:\n\n    df_test = load_df_test()\n    df_test[TARGET_VAR_NM] = pred\n\n    df_test[[ID_VAR_NM, TARGET_VAR_NM]].to_csv('submission.csv', index=False)\n    \ndef load_df_test() -> DataFrame:\n\n    return read_csv('{}/test.csv'.format(DATA_PATH), usecols=[ID_VAR_NM])","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"test_pred = model.predict(ds_test)\ntest_pred *= 100\nsave_test_pred(test_pred)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]}]}